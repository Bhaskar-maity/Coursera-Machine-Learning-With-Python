hello and welcome in this video we'll be covering k-means clustering so let's get started imagine that you have a customer data set and you need to apply customer segmentation on this historical data customer segmentation is the practice of partitioning a customer base into groups of individuals that have similar characteristics one of the algorithms that can be used for customer segmentation is k-means clustering k-means can group data only unsupervised based on the similarity of customers to each other let's define this technique more formally there are various types of clustering algorithms such as partitioning hierarchical or density based clustering k-means is a type of partitioning clustering that is it divides the data into K non-overlapping subsets or clusters without any cluster internal structure or labels this means it's an unsupervised algorithm objects within a cluster are very similar and objects across different clusters are very different or dissimilar as you can see for using k-means we have to find similar samples for example similar customers now we face a couple of key questions first how can we find the similarity of samples in clustering and then how do we measure how similar two customers are with regard to their demographics though the objective of k-means is to form clusters in such a way that similar samples go into a cluster and dissimilar samples fall into different clusters it can be shown that instead of a similarity metric we can use dissimilarity metrics in other words conventionally the distance of samples from each other is used to shape the clusters so we can say k-means tries to minimize the intra cluster distances and maximize the inter cluster distances now the question is how can we calculate the disability or distance of two cases such as to customers assume that we have two customers will call them customer one and two let's also assume that we have only one feature for each of these two customers and that feature is age we can easily use a specific type of Mankowski distance to calculate the distance of these two customers indeed it is the Euclidean distance what about if we have more than one feature for example age and income for example if we have income and age for each customer we can still use the same formula but this time in a two dimensional space also we can use the same distance matrix for multi-dimensional vectors of course we have to normalize our feature set to get the accurate dissimilarity measure there are other dissimilarity measures as well that can be used for this purpose but it is highly dependent on data type and also the domain that clustering is done for it for example you may use Euclidean distance cosine similarity average distance and so on indeed the similarity measure highly controls how the clusters are formed so it is recommended to understand the domain knowledge of your data set and data type of features and then choose the meaningful distance measurement now let's see how K means clustering works for the sake of simplicity let's assume that our data set has only two features the age and income of customers this means it's a two dimensional space we can show the distribution of customers using a scatterplot the y axis indicates age and the x axis shows income of customers we try to cluster the customer data set into distinct groups or clusters based on these two dimensions in the first step we should determine the number of clusters the key concept of the k-means algorithm is that it randomly picks a center point for each cluster it means we must initial kay which represents number of clusters essentially determining the number of clusters in a data set or K is a hard problem in k-means that we will discuss later for now let's put k equals three here for our sample dataset it is like we have three representative points for our clusters these three data points are called centroids of clusters and should be of same feature size of our customer features set there are two approaches to choose these centroids one we can randomly choose three observations out of the data set and use these observations as the initial means or two we can create three random points as centroids of the clusters which is our choice that is shown in the plot with red color after the initialization step which was defining the centroid of each cluster we have to assign each customer to the closest center for this purpose we have to calculate the distance of each data point or in our case each customer from the centroid points as mentioned before depending on the nature of the data and the purpose for which clustering is being used different measures of distance may be used to place items into clusters therefore you will form a matrix where each row represents the distance of a customer from each centroid it is called the distance matrix the main objective of k-means clustering is to minimize the distance of data points from the centroid of its cluster and maximize the distance from other cluster centroids so in this step we have to find the closest centroid to each data point we can use the distance matrix to find the nearest centroid two data points finding the closest centroids for each data point we assign each data point to that cluster in other words all the customers will fall to a cluster based on their distance from centroids we can easily say that it does not result in good clusters because the centroids were chosen randomly from the first indeed the model would have a high error here errors the total distance of each point from its centroid it can be shown as within cluster sum of squares error intuitively we try to reduce this error it means we should shape clusters in such a way that the total distance of all members of a cluster from its centroid be minimized now the question is how can we turn it into better clusters with less error okay we move centroids in the next step each cluster center will be updated to be the mean for data points in its cluster indeed each centroid moves according to their cluster members in other words the centroid of each of the three clusters becomes the new mean for example if point a coordination is seven point four and three point six and point B features are seven point eight and three point eight the new centroid of this cluster with two points would be the average of them which is seven point six and three point seven now we have new centroids as you can guess once again we will have to calculate the distance of all points from the new centroids the points re clustered and the centroids move again this continues until the centroid is no longer move please note that whenever a centroid moves each points distance to the centroid needs to be measured again yes k-means is an iterative algorithm and we have to repeat steps two to four until the algorithm converges in each iteration it will move the centroids calculate the distances from new centroids and assign data points to the nearest centroid it results in the clusters with minimum error or the most dense clusters however as it is a heuristic algorithm there is no t-that it will converge to the global optimum and the result may depend on the initial clusters it means this algorithm is guaranteed to converge to a result but the result may be a local optimum ie not necessarily the best possible outcome to solve this problem it is common to run the whole process multiple times with different starting conditions this means with randomized starting centroids it may give a better outcome and as the algorithm is usually very fast it wouldn't be any problem to run it multiple times thanks for watching this video [Music] you